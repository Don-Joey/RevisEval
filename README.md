# RevisEval: Improving LLM-as-a-Judge via Response-Adapted References
<!-- Example: Test-Time Scaling Atlas: A Unified Framework for Inference-Time Reasoning in LLMs -->


<div align="center">
  <img src="figs/pipeline.png"/>
  <p><b>Figure 1:</b> Illustration of RevisEval.</p>
</div>

**REVISE-AND-EVALUATION (RevisEval)** is a novel text generation evaluation paradigm via the response-adapted reference. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, REVISEVAL leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation.

## 📢 News and Updates

- **[Apr 2025]** 📌 Our paper is revised on [**Arxiv**](https://arxiv.org/abs/2410.05193)!!!
- **[Feb 2025]** 📌 Our paper is accepted by [ICLR 2025 Poster](https://openreview.net/forum?id=1tBvzOYTLF&referrer)!!!
- **[Oct 2024]** 📌 Our initial paper is on [**Arxiv**](https://arxiv.org/abs/2410.05193)!

---

## 📘 Overview

<!-- Add 1-3 key diagrams or tables summarizing your project -->

<img src="figs/table1.png"/> 

<p align="center"><b>Table 1:</b> Kendall (τ) and Spearman (ρ) correlation results comparing reference-free, reference-based, and RevisEval methods across natural language generation tasks. This table demonstrates that, without human-annotated references, our proposed RevisEval substantially outperforms referencefree and reference-based methods involving both open-source and proprietary LLM-as-a-Judge.</p>

<img src="figs/table2.png"/> 

<p align="center"><b>Table 2:</b> Accuracy of LLM-as-a-Judge on instruction-following preference tasks. Our proposed RevisEval considerably enhances the performance of both open-source and proprietary LLM-as-a-Judge across various general evaluation tasks. Here, D.R. denotes Direct Response to instruction.</p>

<img src="figs/figure1.png"/> 

<p align="center"><b>Figure 1:</b> Comparative analysis of reference-based metrics performance using references generated by HUMAN/GPT-4 and RevisEval on NLG and instruction following benchmarks. RevisEval greatly enhance traditional ref-based metrics, even achieving them comparable to GPT-4-as-a-Judge.</p>


<div align="center">
  <img src="figs/table3.png" width="600"/>
  <p><b>Table 3:</b> Comparative analysis of weak LLM-as-a-Judge and weak LLM-as-a-Reviser+classic metrics on instruction-following tasks. Under the same finetuning training resources, a weak LLMas-a-Reviser combined with classic metrics can produce better results.</p>
</div>

There are more detailed results in our paper~


---

## 🚀 How to Use

### 📦 Installation

1. Clone the repository and navigate to the project folder:

```bash
git clone https://github.com/Don-Joey/RevisEval.git
cd RevisEval
```

2. Create the environment and install dependencies:
   
```bash
conda create -n revisevavl python=3.10 -y
conda activate revisevavl
pip install --upgrade pip
pip install -r requirements.txt
```

### ⚡ Quick Start

Evaluate **RevisEval** on the RewardBench.

```bash
## You should fill your api-key.
run main.ipynb
```


## 📄 Citation

```bibtex
@inproceedings{
zhang2025reviseval,
title={RevisEval: Improving {LLM}-as-a-Judge via Response-Adapted References},
author={Qiyuan Zhang and Yufei Wang and Tiezheng YU and Yuxin Jiang and Chuhan Wu and Liangyou Li and Yasheng Wang and Xin Jiang and Lifeng Shang and Ruiming Tang and Fuyuan Lyu and Chen Ma},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=1tBvzOYTLF}
}
···
