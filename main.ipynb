{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from queue import Queue\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import openai\n",
    "\n",
    "from src.data import load_dataset\n",
    "from src.utils import *\n",
    "from src.prompt import format_judge_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client=OpenAI(api_key='<KEY>')\n",
    "client.api_key='...'\n",
    "client.base_url='...'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_json(os.path.join(\".\\configs\", \"rewardbench.json\"))\n",
    "ds = load_dataset(config[\"dataset_key\"], cache_dir=config[\"instructions_responses_pairs_data_path\"])\n",
    "ds = [_ for _ in ds[\"filtered\"]]\n",
    "new_ds = []\n",
    "for case in ds:\n",
    "    if random.uniform(0, 1) < 0.5:\n",
    "        case[\"response a\"] = case[\"chosen\"]\n",
    "        case[\"response b\"] = case[\"rejected\"]\n",
    "        case[\"choice_label\"] = 1\n",
    "    else:\n",
    "        case[\"response a\"] = case[\"rejected\"]\n",
    "        case[\"response b\"] = case[\"chosen\"]\n",
    "        case[\"choice_label\"] = 2\n",
    "    new_ds.append(case)\n",
    "ds = new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Crawl_thread(threading.Thread):\n",
    "    def __init__(self, thread_id, queue):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        #self.filename = target.replace(\"*\", \"_thread\" + str(self.thread_id) + \".jsonl\")\n",
    "        self.queue = queue\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"start thread:\", self.thread_id)\n",
    "        self.crawl_spider()\n",
    "        print(\"quit thread:\", self.thread_id)\n",
    "    \n",
    "    def crawl_spider(self):\n",
    "        global all_get_data3, candidate_key\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                break\n",
    "            else:\n",
    "                row= self.queue.get()\n",
    "                #msgs=row[\"prompt_new\"]\n",
    "                msgs = row #self.openai_template(row[0], row[1])\n",
    "                \n",
    "                try:\n",
    "                    success = False\n",
    "                    for attempt in range(5):\n",
    "                        try:\n",
    "                            #client.api_key = random.choice(all_keys)\n",
    "                            response = client.chat.completions.create(\n",
    "                                model = \"gpt-4o\",\n",
    "                                messages=msgs, \n",
    "                                temperature=0.0\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            time.sleep(random.randint(1, 30))\n",
    "                            print(f'{e}')\n",
    "                        except openai.error.APIerror:\n",
    "                            time.sleep(random.randint(1, 30))\n",
    "                            print(f\"API GG\")\n",
    "                        else:\n",
    "                            success=True\n",
    "                            break\n",
    "                    if success:\n",
    "                        res = response.choices[0].message.content\n",
    "                        row.append(res)\n",
    "                        \n",
    "                except:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviseval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_v = (\n",
    "    \"Please act as a powerful reviser to revise the response generated by an AI \"\n",
    "    \"assistant to the instruction displayed below. You should revise the response to \"\n",
    "    \"follow the user's instructions and answer the user's instruction better. Your \"\n",
    "    \"revision should focus on accuracy and whether the response honestly/precisely/closely executes the instruction. If the original response \"\n",
    "    \"is good enough, simply output the original response. \\n\"\n",
    "    \"[Instruction]\\n{instruction}\\n\"\n",
    "    \"[The Start of Assistant's Answer]\\n{response_output_1}\\n[The End of Assistant's Answer]\\n\"\n",
    "    \"I also give you another model-generated response, which is not necessarily of\"\n",
    "    \"better quality, as a reference for your revision, and you can draw on its\"\n",
    "    \"strengths and avoid its weaknesses.\\n\"\n",
    "    \"[Another Response]\\n{response_output_2}\\n\"\n",
    "    #\"Do NOT provide any explanation for your response.\"\n",
    "    \"ONLY output the complete revised response without saying anything else.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_revise_answers(instruction, answer_a, answer_b, multi_turn=False, model_modifier=None):\n",
    "    kwargs = {}\n",
    "    \n",
    "    user_prompt = revise_v.format(\n",
    "        instruction=instruction,\n",
    "        response_output_1=answer_a[1][\"content\"],\n",
    "        response_output_2=answer_b[1][\"content\"],\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    # gemini adds what was the system prompt before the content, and has no system prompt\n",
    "\n",
    "\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thread_inputs = []\n",
    "for case in tqdm(ds):\n",
    "\n",
    "    user_prompt = format_revise_answers(case[\"prompt\"], [{\"content\": case[\"prompt\"]}, {\"content\": case[\"response b\"]}], [{\"content\": case[\"prompt\"]}, {\"content\": case[\"response a\"]}], False, None)\n",
    "    thread_inputs.append([{\"role\": \"user\", \"content\": user_prompt}])\n",
    "class Crawl_thread(threading.Thread):\n",
    "    def __init__(self, thread_id, queue):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        self.queue = queue\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"start thread:\", self.thread_id)\n",
    "        self.crawl_spider()\n",
    "        print(\"quit thread:\", self.thread_id)\n",
    "    \n",
    "    def crawl_spider(self):\n",
    "        global all_get_data3, candidate_key\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                break\n",
    "            else:\n",
    "                row= self.queue.get()\n",
    "                #msgs=row[\"prompt_new\"]\n",
    "                msgs = row #self.openai_template(row[0], row[1])\n",
    "                try:\n",
    "                    success = False\n",
    "                    for attempt in range(5):\n",
    "                        try:\n",
    "                            #client.api_key = random.choice(all_keys)\n",
    "                            response = client.chat.completions.create(\n",
    "                                model = \"gpt-4o\",\n",
    "                                messages=msgs, \n",
    "                                temperature=0.3\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            time.sleep(random.randint(1, 30))\n",
    "                            print(f'{e}')\n",
    "                        except openai.error.APIerror:\n",
    "                            time.sleep(random.randint(1, 30))\n",
    "                            print(f\"API GG\")\n",
    "                        else:\n",
    "                            success=True\n",
    "                            break\n",
    "                    if success:\n",
    "                        res = response.choices[0].message.content\n",
    "                        row.append(res)\n",
    "                        \n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "all_data = thread_inputs\n",
    "pageQueue = Queue(len(all_data))\n",
    "for p in all_data:\n",
    "    pageQueue.put(p)\n",
    "print(pageQueue.qsize())\n",
    "\n",
    "crawl_threads = []\n",
    "crawl_name_list = range(50)\n",
    "for thread_id in crawl_name_list:\n",
    "    thread = Crawl_thread(thread_id, pageQueue)\n",
    "    time.sleep(0.5)\n",
    "    thread.start()\n",
    "    crawl_threads.append(thread)\n",
    "for thread in crawl_threads:\n",
    "    thread.join()  # Wait for each thread to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_content = []\n",
    "for case, x in zip(thread_inputs, ds):\n",
    "    x[\"reference\"] = case[-1]\n",
    "    print(case[-1])\n",
    "    new_content.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_reference_v2 = (\n",
    "    \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's \"\n",
    "    \"question better. our evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. And I also give a reliable reference answer, and begin your \"\n",
    "    \"evaluation by comparing the two responses with the reference answer and provide a short explanation. Avoid any position biases and ensure that the order in which \"\n",
    "    \"the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names \"\n",
    "    \"of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"\n",
    "    '\"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better.'\n",
    ")\n",
    "\n",
    "\n",
    "MTBENCH_REFERENCE_V2 = {\n",
    "    \"name\": \"pair-v2\",\n",
    "    \"type\": \"pairwise\",\n",
    "    \"system_prompt\": prompt_reference_v2,\n",
    "    \"prompt_template\": \"[User Question]\\n{question}\\n\\n[Reference Answer]\\n{ref_answer}\\n\\n[The Start of Assistant A's Answer]\\n{answer_a}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{answer_b}\\n[The End of Assistant B's Answer]\",\n",
    "    \"description\": \"Prompt for general questions\",\n",
    "    \"category\": \"general\",\n",
    "    \"output_format\": \"[[A]]\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_judge_reference_answers(instruction, reference, answer_a, answer_b, multi_turn=False, model_modifier=None):\n",
    "    kwargs = {}\n",
    "    \n",
    "    system_prompt = MTBENCH_REFERENCE_V2[\"system_prompt\"]\n",
    "    user_prompt = MTBENCH_REFERENCE_V2[\"prompt_template\"].format(\n",
    "        question=instruction,\n",
    "        ref_answer=reference,\n",
    "        answer_a=answer_a[1][\"content\"],\n",
    "        answer_b=answer_b[1][\"content\"],\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    # gemini adds what was the system prompt before the content, and has no system prompt\n",
    "    \n",
    "\n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "thread_inputs = []\n",
    "for case in tqdm(new_content):\n",
    "    system_prompt, user_prompt = format_judge_reference_answers(case[\"prompt\"], case[\"reference\"], [{\"content\": case[\"prompt\"]}, {\"content\": case[\"response a\"]}], [{\"content\": case[\"prompt\"]}, {\"content\": case[\"response b\"]}], False, None)\n",
    "    thread_inputs.append([{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawl_thread(threading.Thread):\n",
    "    def __init__(self, thread_id, queue):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.thread_id = thread_id\n",
    "        #self.filename = target.replace(\"*\", \"_thread\" + str(self.thread_id) + \".jsonl\")\n",
    "        self.queue = queue\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"start thread:\", self.thread_id)\n",
    "        self.crawl_spider()\n",
    "        print(\"quit thread:\", self.thread_id)\n",
    "    \n",
    "    def crawl_spider(self):\n",
    "        global all_get_data3, candidate_key\n",
    "        while True:\n",
    "            if self.queue.empty():\n",
    "                break\n",
    "            else:\n",
    "                row= self.queue.get()\n",
    "                #msgs=row[\"prompt_new\"]\n",
    "                msgs = row #self.openai_template(row[0], row[1])\n",
    "                \n",
    "                try:\n",
    "                    success = False\n",
    "                    for attempt in range(5):\n",
    "                        try:\n",
    "                            #client.api_key = random.choice(all_keys)\n",
    "                            response = client.chat.completions.create(\n",
    "                                model = \"gpt-4o\",\n",
    "                                messages=msgs, \n",
    "                                temperature=0.1\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            time.sleep(random.randint(1, 30))\n",
    "                            print(f'{e}')\n",
    "                        except openai.error.APIerror:\n",
    "                            time.sleep(random.randint(1, 30))\n",
    "                            print(f\"API GG\")\n",
    "                        else:\n",
    "                            success=True\n",
    "                            break\n",
    "                    if success:\n",
    "                        res = response.choices[0].message.content\n",
    "                        row.append(res)\n",
    "                        \n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "all_data = thread_inputs\n",
    "pageQueue = Queue(len(all_data))\n",
    "for p in all_data:\n",
    "    pageQueue.put(p)\n",
    "print(pageQueue.qsize())\n",
    "\n",
    "crawl_threads = []\n",
    "crawl_name_list = range(50)\n",
    "for thread_id in crawl_name_list:\n",
    "    thread = Crawl_thread(thread_id, pageQueue)\n",
    "    time.sleep(0.5)\n",
    "    thread.start()\n",
    "    crawl_threads.append(thread)\n",
    "for thread in crawl_threads:\n",
    "    thread.join() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA, AB, BA, BB = 0, 0, 0, 0\n",
    "for x, case in zip(thread_inputs, new_content):\n",
    "    #判断Flip\n",
    "    if x[-1] is None:\n",
    "        COUNT+=1\n",
    "        continue\n",
    "    case[\"round2_judgment\"] = x[-1]\n",
    "\n",
    "    \n",
    "    if \"[[A]]\" in case[\"round2_judgment\"] and case[\"choice_label\"] == 1:\n",
    "        AA+=1\n",
    "    elif \"[[B]]\" in case[\"round2_judgment\"] and case[\"choice_label\"] == 2:\n",
    "        BB += 1\n",
    "    elif \"[[B]]\" in case[\"round2_judgment\"] and case[\"choice_label\"] == 1:\n",
    "        AB +=1\n",
    "    elif \"[[A]]\" in case[\"round2_judgment\"] and case[\"choice_label\"] == 2:\n",
    "        BA+=1\n",
    "print(AA,BB,AB,BA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION=[358, 358+456, 358+456+740, 1431]\n",
    "VANILLA_SUBSETS = [0, 0, 0, 0]\n",
    "NEW_SUBSETS = [0, 0, 0, 0]\n",
    "def parttt(nid):\n",
    "    if nid < PARTITION[0]:\n",
    "        return 0\n",
    "    elif nid < PARTITION[1] and nid > PARTITION[0]:\n",
    "        return 1\n",
    "    elif nid < PARTITION[2] and nid > PARTITION[1]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "COUNT=0\n",
    "\n",
    "\n",
    "for x, case in zip(thread_inputs, new_content):\n",
    "    #判断Flip\n",
    "    if x[-1] is None:\n",
    "        COUNT+=1\n",
    "        continue\n",
    "    case[\"round2_judgment\"] = x[-1]\n",
    "\n",
    "    subset = parttt(COUNT)\n",
    "    \n",
    "    if \"[[A]]\" in case[\"round2_judgment\"] and case[\"choice_label\"] == 1:\n",
    "        NEW_SUBSETS[subset]+=1\n",
    "    COUNT+=1\n",
    "\n",
    "print(\"NEW:\", NEW_SUBSETS[0]/358, NEW_SUBSETS[1]/456, NEW_SUBSETS[2]/740, NEW_SUBSETS[3]/1431,sum(NEW_SUBSETS)/len(new_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
